{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**автоматический переводчик на основе рекуррентных нейросетей seq2seq**\n",
    "\n",
    "Евгений Борисов borisov.e@solarl.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(d): return \"{:,.0f}\".format(d).replace(\",\", \" \")\n",
    "def ppr(d): print('записей:', pp(len(d)) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 336 667\n"
     ]
    }
   ],
   "source": [
    "# список фраз на английском с переводом на русский\n",
    "with gzip.open('../data/text/rus-eng/rus.txt.gz', 'rt', encoding='utf-8') as f:\n",
    "    lines = f.read().lower().split('\\n')\n",
    "\n",
    "ppr(lines)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "записей: 10 000\n"
     ]
    }
   ],
   "source": [
    "# фразы упорядоченны по длине, выберем среднюю длину\n",
    "lines = [ lines[i] for i in range(100000,110000) ]\n",
    "ppr(lines)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбираем строки на русские и английские с сохранением порядка\n",
    "\n",
    "# определим специальные символы - начало и конец фразы\n",
    "GO='\\t' # символ <старт>\n",
    "EOS='\\n' # символ <стоп>\n",
    "\n",
    "input_texts  = [ s.split('\\t')[0] for s in lines if s ] \n",
    "target_texts = [ GO + ' ' + s.split('\\t')[1]+ ' ' + EOS for s in lines if s ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# входной и выходной алфавиты\n",
    "input_characters  = sorted(set(' '.join(input_texts)))\n",
    "target_characters = sorted(set(' '.join(target_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# размер входного алфавита\n",
    "num_encoder_tokens = len(input_characters) \n",
    "# максимальная длина входной фразы в символах \n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "\n",
    "# размер выходного алфавита\n",
    "num_decoder_tokens = len(target_characters)\n",
    "# максимальная длина выходной фразы в символах \n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нумеруем символы в алфавите\n",
    "input_token_index = { char:i for i, char in enumerate(input_characters) }\n",
    "target_token_index = { char:i  for i, char in enumerate(target_characters) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## кодируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим статистическую модель порождения текста \n",
    "\n",
    "# Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\n",
    "#     Holger Schwenk, Yoshua Bengio\n",
    "# Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "# 3 Sep 2014\n",
    "# https://arxiv.org/abs/1406.1078\n",
    "\n",
    "\n",
    "# statistical machine translation system (SMT)\n",
    "\n",
    "# the goal of the system (decoder,specifically) is to \n",
    "# find a translation f given a source sentence e, which maximizes\n",
    "\n",
    "# p(f|e) ∝ p(e|f)p(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для каждого примера\n",
    "#   строим таблицу индикаторов  {0,1}\n",
    "#     [ номер символа в строке, номер символа в алфавите ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные энкодера \n",
    "\n",
    "# входная последовательность генерирует выход по схеме many2one \n",
    "#   выход энкодера выкидываем\n",
    "#    используем только его конечное состояние\n",
    "#      первым входом декодера есть служебное слово <пуск>\n",
    "encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for s, input_text in enumerate(input_texts):\n",
    "    for w, c in enumerate(input_text):\n",
    "        encoder_input_data[s, w, input_token_index[c]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные декодера (для целевой последовательности), \n",
    "\n",
    "# память декодера инициализируеться конечным состянием памяти энкодера \n",
    "#   и на вход подаём служебное слово <пуск>    \n",
    "#    далее рекурсивно - очередной выход декодера подаёться на вход и генерирует следующий выход\n",
    "\n",
    "# вход декодера\n",
    "decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "# выход декодера (вход смещённый на один шаг)\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, target_text in enumerate(target_texts):\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512  # размер сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 47)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 67)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 1146880     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  1187840     input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 67)     34371       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,369,091\n",
      "Trainable params: 2,369,091\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(model, \n",
    "#                  show_shapes=True,\n",
    "#                  show_layer_names=True).create(prog='dot',format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model,to_file='result/model.png', show_layer_names=False, show_shapes=True )\n",
    "plot_model(model,to_file='model.png', show_layer_names=True, show_shapes=False )\n",
    "\n",
    "IPython.display.Image('model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model,to_file='model.png' ,show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# IPython.display.Image('model.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f3882457ee0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f3882457ee0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "10000/10000 [==============================] - 7s 652us/sample - loss: 1.1616\n",
      "Epoch 2/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.9371\n",
      "Epoch 3/150\n",
      "10000/10000 [==============================] - 4s 404us/sample - loss: 0.8265\n",
      "Epoch 4/150\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.7701\n",
      "Epoch 5/150\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.7156\n",
      "Epoch 6/150\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.6781\n",
      "Epoch 7/150\n",
      "10000/10000 [==============================] - 4s 409us/sample - loss: 0.6296\n",
      "Epoch 8/150\n",
      "10000/10000 [==============================] - 4s 397us/sample - loss: 0.5936\n",
      "Epoch 9/150\n",
      "10000/10000 [==============================] - 4s 394us/sample - loss: 0.5667\n",
      "Epoch 10/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.5341\n",
      "Epoch 11/150\n",
      "10000/10000 [==============================] - 4s 383us/sample - loss: 0.5104\n",
      "Epoch 12/150\n",
      "10000/10000 [==============================] - 4s 389us/sample - loss: 0.4894\n",
      "Epoch 13/150\n",
      "10000/10000 [==============================] - 4s 396us/sample - loss: 0.4617\n",
      "Epoch 14/150\n",
      "10000/10000 [==============================] - 4s 390us/sample - loss: 0.4411\n",
      "Epoch 15/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.4170\n",
      "Epoch 16/150\n",
      "10000/10000 [==============================] - 4s 398us/sample - loss: 0.3977\n",
      "Epoch 17/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.3774\n",
      "Epoch 18/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.3590\n",
      "Epoch 19/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.3416\n",
      "Epoch 20/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.3244\n",
      "Epoch 21/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.3086\n",
      "Epoch 22/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.2942\n",
      "Epoch 23/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.2798\n",
      "Epoch 24/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.2669\n",
      "Epoch 25/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.2564\n",
      "Epoch 26/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.2445\n",
      "Epoch 27/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.2330\n",
      "Epoch 28/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.2231\n",
      "Epoch 29/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.2140\n",
      "Epoch 30/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.2067\n",
      "Epoch 31/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1998\n",
      "Epoch 32/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.1919\n",
      "Epoch 33/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1843\n",
      "Epoch 34/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1783\n",
      "Epoch 35/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1725\n",
      "Epoch 36/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.1674\n",
      "Epoch 37/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1626\n",
      "Epoch 38/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.1580\n",
      "Epoch 39/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1536\n",
      "Epoch 40/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.1492\n",
      "Epoch 41/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1455\n",
      "Epoch 42/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1419\n",
      "Epoch 43/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1383\n",
      "Epoch 44/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.1349\n",
      "Epoch 45/150\n",
      "10000/10000 [==============================] - 4s 389us/sample - loss: 0.1316\n",
      "Epoch 46/150\n",
      "10000/10000 [==============================] - 4s 396us/sample - loss: 0.1286\n",
      "Epoch 47/150\n",
      "10000/10000 [==============================] - 4s 394us/sample - loss: 0.1255\n",
      "Epoch 48/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.1225\n",
      "Epoch 49/150\n",
      "10000/10000 [==============================] - 4s 389us/sample - loss: 0.1204\n",
      "Epoch 50/150\n",
      "10000/10000 [==============================] - 4s 388us/sample - loss: 0.1169\n",
      "Epoch 51/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.1142\n",
      "Epoch 52/150\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.1113\n",
      "Epoch 53/150\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.1089\n",
      "Epoch 54/150\n",
      "10000/10000 [==============================] - 4s 388us/sample - loss: 0.1060\n",
      "Epoch 55/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.1035\n",
      "Epoch 56/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.1071\n",
      "Epoch 57/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.1040\n",
      "Epoch 58/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0971\n",
      "Epoch 59/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0945\n",
      "Epoch 60/150\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.0920\n",
      "Epoch 61/150\n",
      "10000/10000 [==============================] - 4s 395us/sample - loss: 0.0897\n",
      "Epoch 62/150\n",
      "10000/10000 [==============================] - 4s 388us/sample - loss: 0.0878\n",
      "Epoch 63/150\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.0855\n",
      "Epoch 64/150\n",
      "10000/10000 [==============================] - 4s 393us/sample - loss: 0.0832\n",
      "Epoch 65/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0815\n",
      "Epoch 66/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0793\n",
      "Epoch 67/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0776\n",
      "Epoch 68/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0760\n",
      "Epoch 69/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0735\n",
      "Epoch 70/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0724\n",
      "Epoch 71/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0709\n",
      "Epoch 72/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0691\n",
      "Epoch 73/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0678\n",
      "Epoch 74/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0663\n",
      "Epoch 75/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0649\n",
      "Epoch 76/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0636\n",
      "Epoch 77/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0624\n",
      "Epoch 78/150\n",
      "10000/10000 [==============================] - 4s 390us/sample - loss: 0.0611\n",
      "Epoch 79/150\n",
      "10000/10000 [==============================] - 4s 388us/sample - loss: 0.0598\n",
      "Epoch 80/150\n",
      "10000/10000 [==============================] - 4s 390us/sample - loss: 0.0591\n",
      "Epoch 81/150\n",
      "10000/10000 [==============================] - 4s 390us/sample - loss: 0.0575\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 390us/sample - loss: 0.0567\n",
      "Epoch 83/150\n",
      "10000/10000 [==============================] - 4s 389us/sample - loss: 0.0553\n",
      "Epoch 84/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.0548\n",
      "Epoch 85/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0535\n",
      "Epoch 86/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0528\n",
      "Epoch 87/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0518\n",
      "Epoch 88/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0509\n",
      "Epoch 89/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.0501\n",
      "Epoch 90/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0493\n",
      "Epoch 91/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.0488\n",
      "Epoch 92/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0479\n",
      "Epoch 93/150\n",
      "10000/10000 [==============================] - 4s 389us/sample - loss: 0.0472\n",
      "Epoch 94/150\n",
      "10000/10000 [==============================] - 4s 390us/sample - loss: 0.0468\n",
      "Epoch 95/150\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.0456\n",
      "Epoch 96/150\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.0452\n",
      "Epoch 97/150\n",
      "10000/10000 [==============================] - 4s 389us/sample - loss: 0.0447\n",
      "Epoch 98/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0438\n",
      "Epoch 99/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.0434\n",
      "Epoch 100/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.0429\n",
      "Epoch 101/150\n",
      "10000/10000 [==============================] - 4s 392us/sample - loss: 0.0421\n",
      "Epoch 102/150\n",
      "10000/10000 [==============================] - 4s 400us/sample - loss: 0.0418\n",
      "Epoch 103/150\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.0413\n",
      "Epoch 104/150\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.0410\n",
      "Epoch 105/150\n",
      "10000/10000 [==============================] - 4s 388us/sample - loss: 0.0402\n",
      "Epoch 106/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0398\n",
      "Epoch 107/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0395\n",
      "Epoch 108/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0388\n",
      "Epoch 109/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0385\n",
      "Epoch 110/150\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0382\n",
      "Epoch 111/150\n",
      "10000/10000 [==============================] - 4s 394us/sample - loss: 0.0378\n",
      "Epoch 112/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0371\n",
      "Epoch 113/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0365\n",
      "Epoch 114/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0364\n",
      "Epoch 115/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0359\n",
      "Epoch 116/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0357\n",
      "Epoch 117/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0352\n",
      "Epoch 118/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0349\n",
      "Epoch 119/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0343\n",
      "Epoch 120/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0338\n",
      "Epoch 121/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0338\n",
      "Epoch 122/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0335\n",
      "Epoch 123/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0331\n",
      "Epoch 124/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0329\n",
      "Epoch 125/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0322\n",
      "Epoch 126/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0322\n",
      "Epoch 127/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0320\n",
      "Epoch 128/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0314\n",
      "Epoch 129/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0312\n",
      "Epoch 130/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0311\n",
      "Epoch 131/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0305\n",
      "Epoch 132/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0301\n",
      "Epoch 133/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0304\n",
      "Epoch 134/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0299\n",
      "Epoch 135/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0297\n",
      "Epoch 136/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0292\n",
      "Epoch 137/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0293\n",
      "Epoch 138/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0291\n",
      "Epoch 139/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0289\n",
      "Epoch 140/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0284\n",
      "Epoch 141/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0281\n",
      "Epoch 142/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0283\n",
      "Epoch 143/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0280\n",
      "Epoch 144/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0277\n",
      "Epoch 145/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0277\n",
      "Epoch 146/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0275\n",
      "Epoch 147/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0272\n",
      "Epoch 148/150\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0269\n",
      "Epoch 149/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0268\n",
      "Epoch 150/150\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0265\n",
      "CPU times: user 10min 36s, sys: 9.88 s, total: 10min 46s\n",
      "Wall time: 9min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=100,epochs=150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = { i:char for char,i in input_token_index.items() }\n",
    "reverse_target_char_index = { i:char for char,i in target_token_index.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # генерируем состояние энкодера\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # вход декодера - последовательность из одного слова GO\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index[GO]] = 1.\n",
    "\n",
    "    # выходную последовательность\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    for i in range(max_decoder_seq_length): \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # декодируем символ\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # если очередной символ это EOS\n",
    "        if(sampled_char==EOS): break # то завершаем работу\n",
    "\n",
    "        # обновляем входную последовательность\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # обновляем состояние сети\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashions change quickly.  ->   мода быстро меняется. \n",
      "\n",
      "are those your students?  ->   это ваши студенты? \n",
      "\n",
      "you must get up at six.  ->   ты должен встать в шесть. \n",
      "\n",
      "are you a freshman, too?  ->   ты тоже новичок? \n",
      "\n",
      "i'm the youngest child.  ->   я самый младший ребёнок. \n",
      "\n",
      "the door was left ajar.  ->   дверь останили полуоткрытой. \n",
      "\n",
      "are you blackmailing me?  ->   ты меня шантажируешь? \n",
      "\n",
      "it's a pretty big club.  ->   это довольно большой клуб. \n",
      "\n",
      "tom used to be wealthy.  ->   том был когда-то богат. \n",
      "\n",
      "tom was 13 at the time.  ->   тому в то время было тринадцать. \n",
      "\n",
      "tom just saved my life.  ->   том только что спас мне жизнь. \n",
      "\n",
      "the batteries are dead.  ->   батарейки сдохли. \n",
      "\n",
      "does tom have a bicycle?  ->   у тома есть велосипед? \n",
      "\n",
      "tom also speaks french.  ->   том тоже говорит на французском. \n",
      "\n",
      "let's do this together.  ->   давай сделаем это вместе. \n",
      "\n",
      "tom changed everything.  ->   том всё изменил. \n",
      "\n",
      "we met her by accident.  ->   мы случайно её встретили. \n",
      "\n",
      "tom was just not ready.  ->   том вчера видел мэри. \n",
      "\n",
      "is tom going somewhere?  ->   том куда-то идет? \n",
      "\n",
      "this is highly unusual.  ->   это весьма необычно. \n",
      "\n",
      "tom is no longer alone.  ->   том уже не один. \n",
      "\n",
      "i'm still in australia.  ->   я всё ещё в австралии. \n",
      "\n",
      "your door was unlocked.  ->   у тебя дверь была незаперта. \n",
      "\n",
      "tom said he isn't sick.  ->   том сказал, что ему страшно. \n",
      "\n",
      "tom has lost 30 pounds.  ->   том скинул 30 фунтов. \n",
      "\n",
      "these bananas went bad.  ->   эти бананы сгнили. \n",
      "\n",
      "three men were wounded.  ->   три человека было ранено. \n",
      "\n",
      "you're wasting my time.  ->   вы напрасно тратите моё время. \n",
      "\n",
      "no one was watching us.  ->   за нами никто не наблюдал. \n",
      "\n",
      "it's across the street.  ->   это через дорогу. \n",
      "\n",
      "my daughter's your age.  ->   моей дочери столько же лет, сколько вам. \n",
      "\n",
      "if you stay, i'll stay.  ->   если ты останешься, я останусь. \n",
      "\n",
      "they were very popular.  ->   они были очень популярны. \n",
      "\n",
      "tom is living here now.  ->   том сейчас живёт здесь. \n",
      "\n",
      "we'll meet in the park.  ->   встретимся в парке. \n",
      "\n",
      "tom's dreams came true.  ->   мечты тома сбылись. \n",
      "\n",
      "let's get down to work.  ->   давайте приступим к работе. \n",
      "\n",
      "your friend is correct.  ->   ваш друг прав. \n",
      "\n",
      "what else did tom want?  ->   чего ещё том хотел? \n",
      "\n",
      "nothing happened today.  ->   сегодня ничего не происходило. \n",
      "\n",
      "this must be tom's cat.  ->   это, должно быть, кошка тома. \n",
      "\n",
      "you seem to be in love.  ->   ты, кажется, влюбилась. \n",
      "\n",
      "laughter is infectious.  ->   смех заразителен. \n",
      "\n",
      "tom is with his mother.  ->   том с мамой. \n",
      "\n",
      "he nodded encouragingly.  ->   он обнадёживающе кивнул головой. \n",
      "\n",
      "he quietly said goodbye.  ->   он спокойно попрощался. \n",
      "\n",
      "did you get my messages?  ->   вы получили мои сообщения? \n",
      "\n",
      "what else did you want?  ->   чего ещё вы хотели? \n",
      "\n",
      "has everyone gone crazy?  ->   все что, с ума сошли? \n",
      "\n",
      "i'm allergic to gluten.  ->   у меня аллергия на глютен. \n",
      "\n",
      "tom feels the same way.  ->   том чувствует себя точно так же. \n",
      "\n",
      "he behaves like a child.  ->   он ведёт себя как ребёнок. \n",
      "\n",
      "they're very dangerous.  ->   они очень опасны. \n",
      "\n",
      "now, help me with this.  ->   а теперь помогите мне с этим. \n",
      "\n",
      "is there any objection?  ->   есть возражения? \n",
      "\n",
      "tom is mary's gardener.  ->   том - садовник мэри. \n",
      "\n",
      "none of us are perfect.  ->   никто из нас не совершенен. \n",
      "\n",
      "tom drives a black car.  ->   том водит чёрную машину. \n",
      "\n",
      "let's hope tom is home.  ->   будем надеяться, том дома. \n",
      "\n",
      "tom rejected this plan.  ->   том вернулся в бостон. \n",
      "\n",
      "the laughter died down.  ->   смех стих. \n",
      "\n",
      "tom put on his glasses.  ->   том надел очки. \n",
      "\n",
      "tom doesn't sleep here.  ->   том не спит здесь. \n",
      "\n",
      "do you like to watch tv?  ->   вам нравится смотреть телевизор? \n",
      "\n",
      "all our attempts failed.  ->   все наши попытки терпели неудачу. \n",
      "\n",
      "tom can't believe this.  ->   том не может ничего с собой поделать. \n",
      "\n",
      "tom isn't an alcoholic.  ->   том не алкоголик. \n",
      "\n",
      "tom has more questions.  ->   у тома есть ещё вопросы. \n",
      "\n",
      "that'll cause problems.  ->   это доставит проблем. \n",
      "\n",
      "what's your dad's name?  ->   как твоего папу зовут? \n",
      "\n",
      "tom has a half-brother.  ->   у тома есть сводный брат. \n",
      "\n",
      "he made his son a chair.  ->   он сделал своему сыну стул. \n",
      "\n",
      "it looks good on paper.  ->   на бумаге это выглядит не брат. \n",
      "\n",
      "where were you in 2013?  ->   где вы были в две тысячи тринадцатом? \n",
      "\n",
      "we could've helped you.  ->   мы могли бы оказать тебе помощь. \n",
      "\n",
      "you should be a writer.  ->   вам надо быть писателем. \n",
      "\n",
      "tom shuffled the cards.  ->   том перетасовал карты. \n",
      "\n",
      "the wind began to blow.  ->   подул ветер. \n",
      "\n",
      "you have to believe us.  ->   ты должен нам верить. \n",
      "\n",
      "tom was ashamed of you.  ->   тому было за тебя стыдно. \n",
      "\n",
      "i'm not that surprised.  ->   я не особенно удивлён. \n",
      "\n",
      "did you bring any books?  ->   ты принёс книги? \n",
      "\n",
      "are you working tonight?  ->   ты сегодня вечером работаешь? \n",
      "\n",
      "he didn't want to do it.  ->   он не хотел этого делать. \n",
      "\n",
      "my friend is seventeen.  ->   моему другу семнадцать лет. \n",
      "\n",
      "my car has been stolen.  ->   мои кошки будут от этого в восторге. \n",
      "\n",
      "has everyone gone crazy?  ->   все что, с ума сошли? \n",
      "\n",
      "have your clothes dried?  ->   у вас одежда высохла? \n",
      "\n",
      "have you been listening?  ->   ты слушал? \n",
      "\n",
      "your children love you.  ->   ваши дети вас любяз. \n",
      "\n",
      "ask me anything anytime.  ->   спрашивайте меня всё что угодно в любое время. \n",
      "\n",
      "knead the dough gently.  ->   осторожно за нам позедет. \n",
      "\n",
      "what's your waist size?  ->   какой у тебя размер талии? \n",
      "\n",
      "tom was home all night.  ->   том всю ночь был дома. \n",
      "\n",
      "always obey your father.  ->   всегда слушай своего отца. \n",
      "\n",
      "tom needs your support.  ->   тому нужна ваша поддержка. \n",
      "\n",
      "you're in grave danger.  ->   ты в серьёзной опасности. \n",
      "\n",
      "it was quite fantastic.  ->   было просто великолепно. \n",
      "\n",
      "did you come here alone?  ->   ты пришёл сюда один? \n",
      "\n",
      "has tom eaten lunch yet?  ->   том уже пообедал? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ii = np.random.permutation(len(encoder_input_data))[:100]\n",
    "for seq_index in ii:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print( input_texts[seq_index],' -> ', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.0.0\n",
      "keras: 2.2.4-tf\n",
      "GPU devices:\n",
      "   [['/device:GPU:0', 'device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1']]\n",
      "default GPU device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow:', tf.__version__)\n",
    "print('keras:', keras.__version__)\n",
    "\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print('GPU devices:\\n  ',\n",
    "        [ [x.name, x.physical_device_desc] \n",
    "          for x in device_lib.list_local_devices() \n",
    "          if x.device_type == 'GPU' ]\n",
    "    )\n",
    "    print('default GPU device:', tf.test.gpu_device_name() )\n",
    "\n",
    "else:\n",
    "    print('no GPU device found')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
